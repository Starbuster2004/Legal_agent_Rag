import streamlit as st
from config import cfg
from db_store import chroma_client, list_all_documents

st.set_page_config(
    page_title="Legal RAG System",
    page_icon="âš–ï¸",
    layout="wide"
)

def init_state():
    if 'client' not in st.session_state:
        st.session_state.client = chroma_client()

def main():
    st.title('âš–ï¸ Legal RAG System')
    st.markdown('### AI-Powered Legal Document Analysis')
    
    init_state()
    
    # Welcome section
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ## ğŸ’¬ Chat Interface
        
        Ask questions about indexed legal documents using our intelligent RAG system.
        
        **Features:**
        - ğŸ¤– Conversational AI chatbot
        - ğŸ“š Search across all documents
        - ğŸ” Accurate source citations
        - âš¡ Fast responses with Groq
        
        ğŸ‘‰ **[Go to Chat â†’](./1_ğŸ’¬_Chat)**
        """)
    
    with col2:
        st.markdown("""
        ## ğŸ” Admin Panel
        
        Manage your document database with full control.
        
        **Features:**
        - ğŸ“¤ Upload multiple PDFs
        - ğŸ—‚ï¸ Automatic indexing
        - ğŸ“Š View statistics
        - ğŸ—‘ï¸ Delete documents
        
        ğŸ‘‰ **[Go to Admin Panel â†’](./2_ğŸ”_Admin)**
        """)
    
    st.markdown('---')
    
    # Statistics
    st.header('ğŸ“Š System Overview')
    try:
        docs = list_all_documents(st.session_state.client)
        total_chunks = sum(doc['chunk_count'] for doc in docs) if docs else 0
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric('ğŸ“š Indexed Documents', len(docs))
        with col2:
            st.metric('ğŸ§© Total Chunks', total_chunks)
        with col3:
            st.metric('ğŸ¤— Embedding Model', cfg.HUGGINGFACE_EMBED_MODEL.split('/')[-1])
    except Exception as e:
        st.error(f'Error loading statistics: {str(e)}')
    
    # Technology stack
    st.markdown('---')
    st.header('ğŸ› ï¸ Technology Stack')
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("""
        **ğŸ¤— Hugging Face**
        - Embeddings: Local models
        - Reranking: Cross-encoder
        - Privacy-focused
        """)
    with col2:
        st.markdown("""
        **âš¡ Groq**
        - LLM: Llama 3.3 70B
        - Ultra-fast inference
        - High quality answers
        """)
    with col3:
        st.markdown("""
        **ğŸ’¾ ChromaDB**
        - Vector database
        - Persistent storage
        - Per-document collections
        """)

if __name__ == '__main__':
    main()
